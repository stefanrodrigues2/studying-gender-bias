{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,TextDataset, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a Pandas dataframe\n",
    "df = pd.read_csv('careers_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanrodrigues/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanrodrigues/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset and convert it to a PyTorch dataset\n",
    "text_dataset = TextDataset(tokenizer=tokenizer, file_path=\"careers_single.csv\", block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.9 * len(text_dataset))\n",
    "val_size = len(text_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(text_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False, \n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\", \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    data_collator=data_collator, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanrodrigues/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8048\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1509\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0e16cf9596488c8787d5dbf721c1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9148, 'learning_rate': 1.9867461895294898e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9312, 'learning_rate': 1.9734923790589798e-05, 'epoch': 0.04}\n",
      "{'loss': 0.753, 'learning_rate': 1.960238568588469e-05, 'epoch': 0.06}\n",
      "{'loss': 0.247, 'learning_rate': 1.946984758117959e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1668, 'learning_rate': 1.9337309476474488e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1206, 'learning_rate': 1.9204771371769388e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0917, 'learning_rate': 1.907223326706428e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0746, 'learning_rate': 1.893969516235918e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0536, 'learning_rate': 1.8807157057654078e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0419, 'learning_rate': 1.8674618952948974e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0399, 'learning_rate': 1.854208084824387e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0412, 'learning_rate': 1.8409542743538767e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0352, 'learning_rate': 1.8277004638833667e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0311, 'learning_rate': 1.8144466534128564e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0314, 'learning_rate': 1.801192842942346e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0255, 'learning_rate': 1.7879390324718357e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0247, 'learning_rate': 1.7746852220013254e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0302, 'learning_rate': 1.7614314115308154e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0197, 'learning_rate': 1.748177601060305e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0203, 'learning_rate': 1.7349237905897947e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0235, 'learning_rate': 1.7216699801192843e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0196, 'learning_rate': 1.7084161696487743e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0149, 'learning_rate': 1.695162359178264e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0217, 'learning_rate': 1.6819085487077537e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0169, 'learning_rate': 1.6686547382372433e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0209, 'learning_rate': 1.655400927766733e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0175, 'learning_rate': 1.642147117296223e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0253, 'learning_rate': 1.6288933068257123e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0127, 'learning_rate': 1.6156394963552023e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0142, 'learning_rate': 1.602385685884692e-05, 'epoch': 0.6}\n",
      "{'loss': 0.015, 'learning_rate': 1.589131875414182e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0104, 'learning_rate': 1.5758780649436713e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0108, 'learning_rate': 1.5626242544731613e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0166, 'learning_rate': 1.549370444002651e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0125, 'learning_rate': 1.5361166335321406e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0138, 'learning_rate': 1.5228628230616302e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0122, 'learning_rate': 1.50960901259112e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0129, 'learning_rate': 1.4963552021206099e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0108, 'learning_rate': 1.4831013916500995e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0118, 'learning_rate': 1.4698475811795892e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0112, 'learning_rate': 1.4565937707090789e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0125, 'learning_rate': 1.4433399602385687e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0087, 'learning_rate': 1.4300861497680585e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0123, 'learning_rate': 1.4168323392975483e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0118, 'learning_rate': 1.4035785288270378e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0108, 'learning_rate': 1.3903247183565277e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0135, 'learning_rate': 1.3770709078860173e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0148, 'learning_rate': 1.3638170974155071e-05, 'epoch': 0.95}\n",
      "{'loss': 0.008, 'learning_rate': 1.3505632869449966e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Configuration saved in ./checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0077, 'learning_rate': 1.3373094764744865e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac9781ef714046b66644b9bc293e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0021900897845625877, 'eval_runtime': 98.3567, 'eval_samples_per_second': 9.1, 'eval_steps_per_second': 0.569, 'epoch': 1.0}\n",
      "{'loss': 0.0123, 'learning_rate': 1.3240556660039763e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0065, 'learning_rate': 1.3108018555334661e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0058, 'learning_rate': 1.2975480450629556e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0125, 'learning_rate': 1.2842942345924454e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0086, 'learning_rate': 1.2710404241219351e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0073, 'learning_rate': 1.257786613651425e-05, 'epoch': 1.11}\n",
      "{'loss': 0.009, 'learning_rate': 1.2445328031809146e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0112, 'learning_rate': 1.2312789927104042e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0066, 'learning_rate': 1.218025182239894e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0095, 'learning_rate': 1.2047713717693839e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0068, 'learning_rate': 1.1915175612988734e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0083, 'learning_rate': 1.1782637508283632e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0107, 'learning_rate': 1.165009940357853e-05, 'epoch': 1.25}\n",
      "{'loss': 0.01, 'learning_rate': 1.1517561298873427e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0059, 'learning_rate': 1.1385023194168325e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0104, 'learning_rate': 1.125248508946322e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0082, 'learning_rate': 1.1119946984758118e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0106, 'learning_rate': 1.0987408880053017e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0072, 'learning_rate': 1.0854870775347915e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0052, 'learning_rate': 1.072233267064281e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0104, 'learning_rate': 1.0589794565937708e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0082, 'learning_rate': 1.0457256461232605e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0103, 'learning_rate': 1.0324718356527503e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0064, 'learning_rate': 1.01921802518224e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0055, 'learning_rate': 1.0059642147117296e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0055, 'learning_rate': 9.927104042412194e-06, 'epoch': 1.51}\n",
      "{'loss': 0.0081, 'learning_rate': 9.794565937707091e-06, 'epoch': 1.53}\n",
      "{'loss': 0.0098, 'learning_rate': 9.66202783300199e-06, 'epoch': 1.55}\n",
      "{'loss': 0.0072, 'learning_rate': 9.529489728296886e-06, 'epoch': 1.57}\n",
      "{'loss': 0.0047, 'learning_rate': 9.396951623591784e-06, 'epoch': 1.59}\n",
      "{'loss': 0.0068, 'learning_rate': 9.26441351888668e-06, 'epoch': 1.61}\n",
      "{'loss': 0.0072, 'learning_rate': 9.131875414181577e-06, 'epoch': 1.63}\n",
      "{'loss': 0.0094, 'learning_rate': 8.999337309476476e-06, 'epoch': 1.65}\n",
      "{'loss': 0.0076, 'learning_rate': 8.866799204771372e-06, 'epoch': 1.67}\n",
      "{'loss': 0.0048, 'learning_rate': 8.73426110006627e-06, 'epoch': 1.69}\n",
      "{'loss': 0.0079, 'learning_rate': 8.601722995361167e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0079, 'learning_rate': 8.469184890656065e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0062, 'learning_rate': 8.336646785950962e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0076, 'learning_rate': 8.204108681245858e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0076, 'learning_rate': 8.071570576540757e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0082, 'learning_rate': 7.939032471835653e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0104, 'learning_rate': 7.80649436713055e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0041, 'learning_rate': 7.673956262425448e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0058, 'learning_rate': 7.541418157720345e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0087, 'learning_rate': 7.408880053015243e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0079, 'learning_rate': 7.27634194831014e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0067, 'learning_rate': 7.143803843605037e-06, 'epoch': 1.93}\n",
      "{'loss': 0.0075, 'learning_rate': 7.011265738899934e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0047, 'learning_rate': 6.878727634194832e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "Configuration saved in ./checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'learning_rate': 6.7461895294897285e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fe9f1136be4e129e26cdf193159c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.001569723361171782, 'eval_runtime': 97.2229, 'eval_samples_per_second': 9.206, 'eval_steps_per_second': 0.576, 'epoch': 2.0}\n",
      "{'loss': 0.0045, 'learning_rate': 6.613651424784626e-06, 'epoch': 2.01}\n",
      "{'loss': 0.0049, 'learning_rate': 6.4811133200795225e-06, 'epoch': 2.03}\n",
      "{'loss': 0.0041, 'learning_rate': 6.348575215374421e-06, 'epoch': 2.05}\n",
      "{'loss': 0.0038, 'learning_rate': 6.216037110669318e-06, 'epoch': 2.07}\n",
      "{'loss': 0.0045, 'learning_rate': 6.083499005964215e-06, 'epoch': 2.09}\n",
      "{'loss': 0.0059, 'learning_rate': 5.950960901259113e-06, 'epoch': 2.11}\n",
      "{'loss': 0.0057, 'learning_rate': 5.81842279655401e-06, 'epoch': 2.13}\n",
      "{'loss': 0.007, 'learning_rate': 5.685884691848907e-06, 'epoch': 2.15}\n",
      "{'loss': 0.007, 'learning_rate': 5.5533465871438045e-06, 'epoch': 2.17}\n",
      "{'loss': 0.0061, 'learning_rate': 5.420808482438702e-06, 'epoch': 2.19}\n",
      "{'loss': 0.0049, 'learning_rate': 5.2882703777335986e-06, 'epoch': 2.21}\n",
      "{'loss': 0.005, 'learning_rate': 5.155732273028497e-06, 'epoch': 2.23}\n",
      "{'loss': 0.0087, 'learning_rate': 5.023194168323393e-06, 'epoch': 2.25}\n",
      "{'loss': 0.0065, 'learning_rate': 4.890656063618291e-06, 'epoch': 2.27}\n",
      "{'loss': 0.0065, 'learning_rate': 4.758117958913188e-06, 'epoch': 2.29}\n",
      "{'loss': 0.0065, 'learning_rate': 4.625579854208086e-06, 'epoch': 2.31}\n",
      "{'loss': 0.0056, 'learning_rate': 4.493041749502982e-06, 'epoch': 2.33}\n",
      "{'loss': 0.0052, 'learning_rate': 4.36050364479788e-06, 'epoch': 2.35}\n",
      "{'loss': 0.0047, 'learning_rate': 4.227965540092777e-06, 'epoch': 2.37}\n",
      "{'loss': 0.0054, 'learning_rate': 4.095427435387675e-06, 'epoch': 2.39}\n",
      "{'loss': 0.0035, 'learning_rate': 3.962889330682572e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0056, 'learning_rate': 3.830351225977469e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0057, 'learning_rate': 3.697813121272366e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0058, 'learning_rate': 3.5652750165672635e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0072, 'learning_rate': 3.4327369118621605e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0045, 'learning_rate': 3.300198807157058e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0035, 'learning_rate': 3.1676607024519553e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0115, 'learning_rate': 3.0351225977468523e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0072, 'learning_rate': 2.9025844930417498e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0043, 'learning_rate': 2.7700463883366468e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0057, 'learning_rate': 2.637508283631544e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0076, 'learning_rate': 2.504970178926441e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0046, 'learning_rate': 2.372432074221339e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0046, 'learning_rate': 2.239893969516236e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0077, 'learning_rate': 2.1073558648111335e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0084, 'learning_rate': 1.9748177601060305e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0033, 'learning_rate': 1.842279655400928e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0058, 'learning_rate': 1.7097415506958251e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0049, 'learning_rate': 1.5772034459907224e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0041, 'learning_rate': 1.4446653412856198e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0054, 'learning_rate': 1.312127236580517e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0054, 'learning_rate': 1.1795891318754142e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0078, 'learning_rate': 1.0470510271703117e-06, 'epoch': 2.84}\n",
      "{'loss': 0.007, 'learning_rate': 9.145129224652089e-07, 'epoch': 2.86}\n",
      "{'loss': 0.0057, 'learning_rate': 7.819748177601061e-07, 'epoch': 2.88}\n",
      "{'loss': 0.0082, 'learning_rate': 6.494367130550033e-07, 'epoch': 2.9}\n",
      "{'loss': 0.0061, 'learning_rate': 5.168986083499006e-07, 'epoch': 2.92}\n",
      "{'loss': 0.0056, 'learning_rate': 3.843605036447979e-07, 'epoch': 2.94}\n",
      "{'loss': 0.0046, 'learning_rate': 2.518223989396952e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "Configuration saved in ./checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'learning_rate': 1.1928429423459245e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f71a97c3faa407cad5157151a88a524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0013568727299571037, 'eval_runtime': 91.8732, 'eval_samples_per_second': 9.742, 'eval_steps_per_second': 0.61, 'epoch': 3.0}\n",
      "{'train_runtime': 14997.2764, 'train_samples_per_second': 1.61, 'train_steps_per_second': 0.101, 'train_loss': 0.051731910956486556, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1509, training_loss=0.051731910956486556, metrics={'train_runtime': 14997.2764, 'train_samples_per_second': 1.61, 'train_steps_per_second': 0.101, 'train_loss': 0.051731910956486556, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in fine-tuned-t5-base/config.json\n",
      "Configuration saved in fine-tuned-t5-base/generation_config.json\n",
      "Model weights saved in fine-tuned-t5-base/pytorch_model.bin\n",
      "tokenizer config file saved in fine-tuned-t5-base/tokenizer_config.json\n",
      "Special tokens file saved in fine-tuned-t5-base/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine-tuned-t5-base/tokenizer_config.json',\n",
       " 'fine-tuned-t5-base/special_tokens_map.json',\n",
       " 'fine-tuned-t5-base/spiece.model',\n",
       " 'fine-tuned-t5-base/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine-tuned-t5-base')\n",
    "tokenizer.save_pretrained('fine-tuned-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanrodrigues/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained and fine-tuned T5 models\n",
    "pt_model_name = \"t5-base\"\n",
    "pt_tokenizer = T5Tokenizer.from_pretrained(pt_model_name)\n",
    "pt_model = T5ForConditionalGeneration.from_pretrained(pt_model_name)\n",
    "ft_model_name = \"fine-tuned-t5-base\"\n",
    "ft_tokenizer = T5Tokenizer.from_pretrained(ft_model_name)\n",
    "ft_model = T5ForConditionalGeneration.from_pretrained(ft_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input prompt\n",
    "prompt = \"My dad applied for the position of kindergarten teacher.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Output\n",
    "def generate_output(prompt, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # Generate text completion\n",
    "    output_ids = model.generate(input_ids, max_length=100, do_sample=True, temperature=1.5, top_k=50,repetition_penalty=2.0, top_p=0.95)\n",
    "    # Decode the generated tokens back into text\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: My dad applied for the position of kindergarten teacher.\n",
      "Pre-trained T5 output: On December 25, I turned 14. Many thanks for your post.\n",
      "Fine-tuned T5 output: (My dad applied for the position of kindergarten teacher.) My dad applied from an educational background in England during his first school year to become a teacher.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"My dad applied for the position of kindergarten teacher.\"\n",
    "prompts = [\n",
    "    \"My dad applied for the position of kindergarten teacher.\"\n",
    "]\n",
    "for prompt in prompts:\n",
    "    print(\"Prompt:\", prompt)\n",
    "    pt_output = generate_output(prompt, pt_model, pt_tokenizer)\n",
    "    print(\"Pre-trained T5 output:\", pt_output)\n",
    "    ft_output = generate_output(prompt, ft_model, ft_tokenizer)\n",
    "    print(\"Fine-tuned T5 output:\", ft_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a4464dfeb09d8bb859ff233b987e5a8086d95f8aa46b6582b0948e817a56f8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
